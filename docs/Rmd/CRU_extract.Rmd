---
title: CRU 3.1 Data Extraction Code
author: Matthew Leonawicz
output:
  html_document:
    toc: TRUE
    theme: cosmo
    highlight: zenburn
    keep_md: TRUE
---

```{r knitr_setup, echo=FALSE}
opts_chunk$set(cache=FALSE, eval=FALSE, tidy=TRUE, messages=FALSE, warnings=FALSE)
read_chunk("../../code/code_sankey.R")
read_chunk("../../code/CRU_extract.R")
```

## Introduction

This script extracts CRU 3.1 temperature and precipitation data from regional shapefiles and the full Alaska-Canada 2-km extent.
Optionally it also extracts data for specific point locations via the raster grid cell in which given spatial coordinates fall.
Data are saved to **R** workspaces (.RData files) for analysis and graphing by subsequent **R** code.

### Motivation
The primary motivation for this code is not just to extract regional and point data from a large number of high-resolution geotiffs,
but to limit the routine recurrence and redundancy of such extractions.
It allows for storing commonly required data in a more compact format that can be quickly shared and digested by other projects.

### Details

#### Capabilities
This script is set up to use parallel processing via the base `parallel` package and the function `mclapply` similar to its companion script, `AR4_AR5_extract.R`.
However, there are some subtle differences. First, it processes precipitation and temperature back to back, serially.
Between this and the fact that CRU 3.1 is a single data set, compared to the multiple climate models and scenarios, there is nothing remaining to parallelize across.
The `mclapply` call only runs the extraction function on a single CPU core.
But in the future this code can easily be extended in parallel across more than just CRU 3.1, should other comparable data products become available.

The code ran be run strictly to obtain community data, regional data, or both. This allows for redoing only one type of extraction, for instance if new shapefiles or additional city locations are desired.
It can be passed an arbitrary list of cities and/or regions.

#### Limitations
This code is typically called by a slurm script. It requires no command line arguments.
Although it uses only a single CPU core, it is still run as its own slurm job, whereas it might be better to fold `CRU_extract.slurm` in with `AR4_AR5_extract.slurm`.
It is not set up to parallelize across climate variables.

Cities coordinates are assumed to be WGS84 Lat/Lon and are projected to NAD 83 Alaska Albers prior to use in extraction.
Regional information are expected in the form of a named nested list object of a specific type.
Non-list type sub-elements are vectors of cell indices previously extracted from 2-km Alaska-Canada extent template rasters using named groups of various shapefiles.
These objects are created in advance. There are multiple versions, as indexing by shapefile is based on the origin, resolution and extent of the rasters and geotiffs of intended use,
as well as subsampling and/or NA-removal, if desired.
The specific **R** workspace intended for use in data extraction by this script is hard coded and matches that used in `AR4_AR5_extract.R`.

### Files and Data
The input files include 2-km Alaska-Canada extent data from 10 combined CMIP3 and CMIP5 downscaled GCMs
as well as community locations and raster layer cell indices pertaining to various regional shapefile polygons.
`CRU_extract.R` is called via slurm script, `CRU_extract.slurm`.

The `CRU_extract.R` script produces three main sets of output data:
* Data for variables at the scale and location of individual communities
* Spatially aggregated summary statistics of this data over larger regions
* Regional probability distribution estimates for these same regions.

Each of these groups of outputs is handled separately by additional **R** scripts.

In the hierarchy of these extraction and data preparation scripts, `AR4_AR5_extract.R` exists alongside `CRU_extract.R` which performs similar operations on downscaled GCM data.

### Code flow

The collection of project code of which this script is a part exists within the context of a broader hierarchy of **R** code and data sets (shown below) spanning a multitude of other projects (not shown).

```{r code_sankey, echo=F, eval=T}
```

```{r code_sankey_embed, echo=F, eval=T, comment = NA, results = 'asis', tidy = F}
```


## **R** Code

### Setup

Setup consists of loading required **R** packages and additional files, preparing any command line arguments for use, and defining functions.

#### Required packages

```{r packages}
```

#### Command line arguments

```{r clargs}
```

#### Sourced code

```{r source}
```

#### Define **R** objects

```{r setup}
```

#### Support functions

```{r functions1}
```

----

```{r functions2}
```

### Processing

Compile annual average time series data for each variable serially.
`mclapply` is called here in keeping with the method applied in the corresponding GCM extraction code found in `AR4_AR5_extract.R`, but defaults to serial processing.

```{r run}
```

### Results

```{r save}
```
