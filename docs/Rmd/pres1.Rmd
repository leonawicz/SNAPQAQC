---
title: "Working with data"
author: Matthew Leonawicz
output:
  ioslides_presentation:
    widescreen: true
    logo: SNAP_acronym_for-dark-bkgrnd_100px.png
    css: pres1_styles.css
  slidy_presentation:
    theme: united
    highlight: haddock
    font_adjustment: -1
    css: pres1_styles.css
  beamer_presentation:
    theme: "AnnArbor"
    colortheme: "dolphin"
    fonttheme: "structurebold"
---

```{r knitr_setup, echo=FALSE, results="hide", message=FALSE, warning=FALSE}
opts_chunk$set(cache=FALSE, eval=TRUE, echo=FALSE, tidy=TRUE, message=FALSE, warning=FALSE, dpi=72, fig.width=5, fig.height=3)
set.seed(47)
source("../../code/alfresco/functions.R")
if(!exists("lb") | !exists("ub")) { lb <- 0.025; ub <- 0.975 } # confidence limits
lapply(c("reshape2", "dplyr", "data.table", "ggplot2", "grid", "tidyr"), library, character.only=T)
load("../../data/presentation.RData")
```

## The three A's

- Access
- Assemble
- Analyze

Data extraction, data prep, and statistical analysis

## Abstraction

- Data prep should be abstracted from any analysis
- "Prep" implies targeted usage, but can be generalized with careful planning
- Extraction is treated separately from subsequent data prep and is most important to abstract from analysis

## Extract once, analyze freely

A series of 100 procedures might consist of 50 heavy duty extraction tasks, each followed by some kind of exploration of the extracted data.

It should consist of one extraction task followed by 99 unique data analyses.

----

Example:

Extract data from 500,000 spatially explicit geotiffs

- Data are tabled and stored efficiently for subsequent access.
- No need to return repeatedly to raw maps to perform redundant extractions.
- Data describe distributions of random variables rather than specific statistics

Organize data for easy analysis

- Final data formats are standardized tables
- General enough to work across multiple types of RV data sets, allowing easy merging and comparison of multiple data sets

Load data into a specific project to analyze


## Statistical modeling: A closer look at data prep
<div class="columns-2">
Probability distributions

Let $X$ be random variable, e.g., stand age, burn area, precipitation, etc.

Model the probability density function (pdf) of $X$, $f(X)$

```{r}
x=seq(-3,3,len=200)
par(mar=c(0,0,0,0))
plot(x, dnorm(x), type="l", axes=F, xlab="X", ylab="density")
abline(v=mean(x), col=2)
text(mean(x), 0.1, "mean", col=2, pos=4)
```
</div>

- Achieve substantial data reduction without effective loss of information
- Any statistic based on the RV can be calculated from its pdf.
- No need to return to raw maps each time a new summary statistic is requested; compute it with the distribution model

----

Data manipulation

- Standardized data table formats
- Easy to work with, easy to read
- Fast operations
- Simple tables can store complex data structures and models, which are easily indexed, extracted or unnested

----

Example data table

```{r}
data.table(data)
```

- Standardized across multiple variables
- Easy to combine and compare

----

Straightforward manipulation, clear code

```{r, tidy=F, echo=TRUE}
data %>% filter(Model=="CCCMAcgcm31") %>% group_by(Scenario, Model, Vegetation) %>%
    summarise(Decade="2010s", Avg=mean(Val)) %>% data.table
```

- Intuitive verbs
- Read left-to-right
- Ideal for standard, human-guided interactive data analysis as well as non-interactive processing

----

Fast operations

- Data table manipulations, restructuring, summarizing and statistical computations are fast
- Comparable to matrix arithmetic with slight overhead
- Core functionality optimized in C++, accessed via Rcpp

----

Organize complex data structures and statistical models in small tables

- Visually compress large data tables to see the big picture at a glance

<div class="columns-2">
```{r, tidy=F, echo=TRUE}
table1 <- data %>% filter(Model=="CCCMAcgcm31" & Year==2010) %>%
    group_by(Scenario, Vegetation) %>%
    do(Density=data.table(Val=Val, Prob=Prob), # column collapse
       Stats=data.frame(as.list(summary(Val))), # summary stats
       Linear_model=lm(Val~I(rnorm(length(Val)))) # complex object
    ) %>% data.table
table1
```
</div>

- Access arbitrary objects from data table cells

```{r, tidy=F, echo=TRUE}
table1$Linear_model[[1]] # Index and extract a specific regression model
```

----

Unnest the Summary column

```{r, tidy=F, echo=TRUE}
table1 %>% select(Scenario, Vegetation, Stats) %>% unnest %>% data.table
```

Unnest the Density column to recover the original table values
```{r, tidy=F, echo=TRUE}
table1 %>% select(-Stats, -Linear_model) %>% unnest %>% data.table
```

## Working with probability distributions

Joint distributions: $f_{X_1,X_2,...,X_n}(X_1,X_2,...,X_n)$

- Multivariate distributions, indirectly represented by the table as a whole
- Relatively trivial in this context since most factors are fixed effects

Conditional distributions: $f_{X,Y}(X|Y=y)$

- Tables are a series of conditional distributions of $X$
- Accessing a conditional distribution simply requires row filtering

Marginal distributions: $f_{X,Y}(X)$

- Accessible by integrating over levels of $Y$

----

Using distribution tables: Layer of abstraction between user and detailed code

<div class="columns-2">
Ex: Condition, marginalize, sample

```{r, tidy=F, echo=TRUE}
ws2014 <- data %>% filter(Vegetation=="White Spruce" & Year==2014)
values <- ws2014 %>% marginalize(margin="Model") %>%
    sample_densities
values2 <- ws2014 %>% marginalize(margin=c("Scenario", "Model")) %>%
    sample_densities
```
Colored lines: $f_{X,Model}(X|Scenario)$

Conditional distributions of $X$ given scenario, marginalized over Model

```{r, fig.width=7, fig.height=5}
ggplot(data=values, aes(x=Val, colour=Scenario)) + geom_line(stat="density", size=1) + geom_line(data=values2, stat="density", colour="black", linetype=2, size=2) + theme(legend.position="bottom", plot.margin=unit(c(1,0,0,0), units="cm"))
```
</div>

Black line: $f_{X,Scenario,Model}(X)$

Marginal distribution of $X$ with respect to Scenario and Model

----

Compare distributions of $X$ defined over an increasing set of interacting factors

```{r, tidy=F, echo=TRUE}
XgivenSM <- ws2014 %>% filter(Scenario=="SRES B1" & Model=="ukmoHADcm3") # condition on specific scenario and model
XgivenS <- ws2014 %>% filter(Scenario=="SRES B1") %>% marginalize("Model") # condition on a scenario, marginalize over models
X <- ws2014 %>% marginalize(c("Scenario", "Model")) # marginalize over scenarios and models
```

<div class="columns-2">
```{r, fig.width=7, fig.height=5}
lev <- c("Condition on both", "Condition on scenario\nmarginalize models", "Marginalize over both")
XgivenSM$Type <- factor(lev[1], lev)
XgivenS$Type <- factor(lev[2], lev)
X$Type <- factor(lev[3], lev)
X2 <- rbindlist(list(XgivenSM, XgivenS, X), fill=T)
ggplot(X2 %>% group_by(Type) %>% sample_densities, aes(x=Val, fill=Type, colour=Type)) + geom_density(size=1, alpha=0.5) + theme(legend.position="bottom")
```

Marginal distributions tend to be wider

Conditioning on fixed levels of variables tends to narrow a distribution

The distributions of $X$ and $Y$ (e.g., Scenario, Model) are not independent or the distribution of $X$ would be the same regardless of the other variables' levels.

</div>

----

Sanity check: Does a distribution "wash out" if manipulated too many times?

<div class="columns-2">
- Sometimes several iterations of distribution merging may be required
- Each merge requires a cycle of sampling from the densities to be merged and estimating a new density from the pooled sample
- If sample is sufficiently large and density sufficiently flexible, no effective information lost

```{r, fig.width=7, fig.height=5}
set.seed(1)
ggplot(X %>% bootDenCycle(10) %>% sample_densities, aes(x=Val, colour=factor(Cycle))) + geom_line(stat="density", size=1) + theme(legend.position="bottom")
```
</div>

- Use bootDenCycle() test function to perform 10 sequential cycles of bootstrap resampling and density re-estimation

## Summarizing uncertainty in a random variable

- Uncertainty can be summarized with a certainty interval
- Uncertainty in $X$ compounds across multiple factors, widening the certainty interval
- Individual uncertainty components by factor
- Proportional uncertainty by factor
- Hierarchical stepwise addition of conditional uncertainty by factor
- Pairwise comparison of uncertainty from a factor due to specific factor levels
- Constraints on maximum uncertainty/minimum certainty

----

Functions to summarise uncertainty include uc_table(), uc_components() and uc_stepwise()

- uc_table provides quick access to certainty bounds
- Bounds are based on a flexible definition, default- or user-provided

```{r}
uc_table(data)
```

## Uncertainty examples

----

<img src="img/presentations/veg_tsByVeg_ucCompound.png" style="width:960px; height:560px;">

----

<img src="img/presentations/veg_tsByVeg_ucComponentStack.png" style="width:960px; height:560px;">

----

<img src="img/presentations/veg_tsByVeg_ucComponentProp.png" style="width:960px; height:560px;">

----

<img src="img/presentations/veg_tsByVeg_ucComponentStack_stepwise.png" style="width:960px; height:560px;">

----

<img src="img/presentations/veg_ucModelPairs.png" style="width:960px; height:560px;">

----

<img src="img/presentations/veg_marCondDensity4.png" style="width:960px; height:560px;">

----

<img src="img/presentations/veg_tsByVeg_ucTotal.png" style="width:960px; height:560px;">


## Analysis driven by specific research questions

- Data exploration and statistical analysis become easier to perform
- Questions become easier to address

----

Inverse probability: from $f(X|Y=y)$ to $f(Y|X=x)$

<div class="columns-2">
Example: First look at a joint distribution of $X$ and $Y$, $f_{X,Y}(X,Y)$

$X$ is total regional black spruce area in km$^2$. $Y$ is Scenario. Other factors fixed.

```{r, echo=FALSE}
X <- filter(data, Model=="CCCMAcgcm31" & Year==2010 & Vegetation=="Black Spruce") %>% data.table
select(X, -Model, -Year)
```

</div>

- The table can be viewed as an indirect representation of the joint distribution of multiple random variables.
- Directly, it is a sequence of conditional distributions of $X$ given unique combinations of factor levels of other variables.
- Marginal distributions of $X$ with respect to a factor, e.g., Scenario or Model, require integrating out the factor.

----

To simplify, classify $X$ into three categories. Now $X$ and $Y$ are both categorical.

<div class="columns-2">
```{r, echo=FALSE}
X <- group_by(X, Scenario) %>% sample_densities %>% group_by(Scenario) %>% summarise(Small=length(which(Val < 85000))/(3*length(Val)),
    Medium=length(which(Val >= 85000 & Val < 97000))/(3*length(Val)),
    Large=length(which(Val >= 97000))/(3*length(Val))) %>% data.frame
rownames(X) <- X$Scenario
X <- as.table(as.matrix(X[,2:4]))[c(3,1,2),]
X <- rbind(cbind(X, Margin=margin.table(X,1)), Margin=c(margin.table(X,2), 1))
kable(X, caption="Joint pmf Spruce Cover & Scenario", digits=4)
```

$$f(X,Y)/f(Y)=f(X|Y=y)$$

$$f(X,Y)/f(X)=f(Y|X=x)$$

<div class="columns-2">
```{r}
kable(X[1,-4] / X[1,4], caption="f(X | Y = SRES B1)", digits=4)
```

```{r}
kable(X[-4,1] / X[4,1], caption="f(Y | X = Small)", digits=4)
```

</div>
</div>

<div class="notes">
To simplify, let's look at a more basic joint probability distribution we can construct from this table.

- It is displayed as a 3 x 3 contingency table, a simple representation of a joint probability mass function where the two variables each have three discrete levels.
- The nine cells in the table are non-negative and sum to one. They represent probabilities of each combination of the variables' values.
- The margins also sum to one. This is where the term marginal distribution comes from; the margins of a contingency table which express the probabilities of levels of a random variable regardless of the level of the other variable.

While area was classified for simplicity, research questions also typically lead to classifying continuous random variables at some point in an analysis in order to make a statement about values in some range of interest.

We may have already used the full table to make statements about the probability that stand age is greater than x, given a specific scenario, or marginalized over all scenarios.
Here the single cell marking that combination of age range and scenario level gives the joint probability.

The conditional probability of this age range given the specific scenario is the joint probability divided by the marginal probability of the specific scenario.
Each scenario has a trivial probability of 1/3 since scenario is a fixed effect.

While we know the probability of a specific scenario is 1/3, uninteresting for a fixed effect, we can see how it can be derived as the joint probability divided by the conditional probability.

Why is this worth mentioning? Knowing this, and looking at the joint probability distribution of X and scenario in a contingency table, it's intuitive to see that we can work in the opposite direction.

While sometimes it is of interest to know the probability that X is greater than x given a specific scenario,
sometimes research questions translate into asking the opposite: what is the probability that we are working with or looking at output from a specific scenario, given our interest in when X is greater than x?

This upper tail of the distribution of X may more likely be seen in some scenarios than others.

Conditional on the values of interest for the random variable X, we have a probability mass function for scenario. This can be applied to any combination of variables.

For example, given stand age of a certain vegetation class in a region is less than 10 years old, or given the area burned in a region exceeds some upper threshold of interest, what is the probability this can be seen under the input conditions of a specific GCM?
</div>

----

<div class="columns-2">
Example: "Bounding models"

- Be specific: Bounding what?
- Be objective: Define conditions. Use math.
- Be rigorous: Formalize methods. Calculate probabilities.

Ad hoc decision-making from purely exploratory graphs by "eyeballing it" lacks mathematical rigor and scientific objectivity.

The graph here is the opposite: a visual representation of an objective decision, based on a formalized methodological framework.

```{r, echo=FALSE, fig.width=7, fig.height=5}
X <- filter(data, Vegetation=="White Spruce") %>% sample_densities %>% group_by(Model) %>%
    summarise(Small=length(which(Val < 92000))/(length(Val)),
        Medium=length(which(Val >= 92000 & Val < 106000))/(length(Val)),
        Large=length(which(Val >= 106000))/(length(Val)))
X <- melt(X, id.vars="Model", variable.name="Area", value.name="Probability") %>% data.table %>% group_by(Area) %>%
    mutate(Probability=Probability/sum(Probability)) %>% filter(Area=="Large") %>% arrange(Probability) %>% mutate(Model=factor(Model, levels=unique(Model)), Area="f(Model | Spruce Cover = Large)")
X$Color <- 2
lev <- levels(X$Model)
X$Color[X$Model==lev[1]] <- 1
X$Color[X$Model==tail(lev, 1)] <- 3
X$Color <- factor(X$Color)
ggplot(X, aes(x=Model, y=Probability, colour=Color)) + geom_bar(stat="identity", size=2) + facet_wrap(~Area) + scale_colour_manual(values=c("dodgerblue", "white", "firebrick")) + guides(colour=FALSE)
```

My favorite: Explain what makes your models "hot and cold" models. Can you articulate it simply and unambiguously right away, or do you have to really think about it for the first time?
</div>

<div class="notes">

Five climate models, but for a project we may only want to work with two of them. Which two do we use?
The ambiguous answer is the same as always: "the hot one and the cold one".
But the time has passed for applying the unsophisticated guesswork that ambiguity demands.
We have to ask what that means. What defines hot and cold? Are we more concerned about climate inputs, and if so, hot vs. cold or dry vs. wet, or are we more concerned about fire outputs?
For some projects it may be more relevant to consider area cover by specific vegetation types.

Boundary conditions should be well defined.
When they are, it becomes easy to use those conditions to obtain the probabilities associated with levels of a factor in question, e.g., GCM, and the two GCMs with the lowest and highest probabilities given the conditions, respectively, are the bounding GCMs.
Trying to distill the choice of models in a case like this from staring an assortment of plots lacks rigor but is also unnecessary when more formal tools are available.
</div>